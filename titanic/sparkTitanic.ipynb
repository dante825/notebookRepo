{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Spark ML example\n",
    "\n",
    "Using the titanic dataset, a simple Spark ML example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Titanic\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option('header', True).load('./titanicData/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory expensive since it loads everything into memory\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the columns are shown as string types, that's not correct.</br>\n",
    "Thus, cast some of the columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataset = df.select(col('Survived').cast('float'),\n",
    "            col('Pclass').cast('float'),\n",
    "            col('Sex'), \n",
    "            col('Age').cast('float'),\n",
    "            col('Fare').cast('float'),\n",
    "            col('Embarked'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check number of null values in columms\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.filter('Age is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in columns\n",
    "dataset = dataset.replace('?', None).dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical columns with StringIndexer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the columns 1 by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Order options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc\n",
    "# handleInvalid options: skip, error, keep\n",
    "# genderIndexer = StringIndexer(inputCol='Sex', outputCol='gender', handleInvalid='keep', stringOrderType='frequencyDesc')\n",
    "# boardedIndexer = StringIndexer(inputCol='Embarked', outputCol='boarded', handleInvalid='keep', stringOrderType='frequencyDesc')\n",
    "\n",
    "# dataset = genderIndexer.fit(dataset).transform(dataset)\n",
    "# dateset = boardedIndexer.fit(dataset).transform(dataset)\n",
    "\n",
    "dataset = StringIndexer(\n",
    "    inputCol = 'Sex',\n",
    "    outputCol = 'Gender',\n",
    "    handleInvalid = 'keep',\n",
    "    stringOrderType='frequencyDesc').fit(dataset).transform(dataset)\n",
    "\n",
    "dataset = StringIndexer(\n",
    "    inputCol = 'Embarked',\n",
    "    outputCol = 'Boarded',\n",
    "    handleInvalid = 'keep',\n",
    "    stringOrderType='frequencyDesc').fit(dataset).transform(dataset)\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index multiple columns at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# StringIndexer also accepts arrays\n",
    "dataset = StringIndexer(inputCols=['Sex', 'Embarked'], outputCols=['Gender', 'Boarded'], handleInvalid='keep', \n",
    "stringOrderType='frequencyDesc').fit(dataset).transform(dataset)\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For StringIndexer, there's no method to retrieve the mapping? </br>\n",
    "Only through ordering by frequency or dependent on the ordering type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupBy('gender', 'sex').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select('gender', 'sex').distinct().orderBy('gender', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.select('embarked', 'boarded').distinct().orderBy('Embarked', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby('embarked', 'boarded').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecesary columns\n",
    "dataset = dataset.drop('Sex')\n",
    "dataset = dataset.drop('Embarked')\n",
    "\n",
    "dataset.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column features\n",
    "\n",
    "Combine all the feature columns (excluding the target column) into 1 vector with <strong>VectorAssembler</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "required_features = ['Pclass', 'Age', 'Fare', 'Gender', 'Boarded']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "\n",
    "transformed_data = assembler.transform(dataset)\n",
    "transformed_data.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = transformed_data.randomSplit([0.8, 0.2])\n",
    "print(training_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='Survived', featuresCol='features', maxDepth=5)\n",
    "model = rf.fit(training_data)\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainingSummary = model.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'], pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metricName options: accuracy, f1, precisionByLabel, recallByLabel\n",
    "eval_acc = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction', metricName='accuracy')\n",
    "eval_pre = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction', metricName='precisionByLabel')\n",
    "eval_rec = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction', metricName='recallByLabel')\n",
    "eval_f1 = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction', metricName='f1')\n",
    "accuracy = eval_acc.evaluate(predictions)\n",
    "precision = eval_pre.evaluate(predictions)\n",
    "recall = eval_rec.evaluate(predictions)\n",
    "f1 = eval_f1.evaluate(predictions)\n",
    "\n",
    "print(f'Accracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'f1 score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = predictions.select(['prediction','Survived']).withColumn('Survived', F.col('Survived').cast(FloatType())).orderBy('prediction')\n",
    "# select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','Survived'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm2 = metrics.confusionMatrix().toArray()\n",
    "disp2 = ConfusionMatrixDisplay(cm2)\n",
    "disp2.plot()\n",
    "plt.title('Confusion Matrix from PySpark')\n",
    "plt.show()\n",
    "\n",
    "# Has error using jupyter, on .py file it is fine, refer to sparkTitanic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: using sklearn confusion matrix\n",
    "# if the data size is huge, sklearn may not be able to handle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_true = predictions.select('Survived').collect()\n",
    "y_pred = predictions.select('prediction').collect()\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(cm, display_labels=['Not Survived', 'Survived'])\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = sc.parallelize([(0.0, 0.0), (0.0, 1.0), (0.0, 0.0), (1.0, 0.0), (1.0, 1.0), (1.0, 1.0)])\n",
    "print(type(predictionAndLabels.collect()))\n",
    "# metrics = MulticlassMetrics(predictionAndLabels)\n",
    "# metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-kernel",
   "language": "python",
   "name": "sparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
